{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"abcd-tools","text":"<p>Welcome to the repository for the abcd-tools project!</p>"},{"location":"#quickstart","title":"Quickstart","text":""},{"location":"#install-from-source","title":"Install from source","text":"<pre><code>pip install git@github.com:ajbarrows/abcd-tools\n</code></pre>"},{"location":"#why-this-project-exists","title":"Why this project exists","text":"<p>Place your reasons here for why this project exists.</p> <p>What benefits does this project give to users?</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"data_loading_and_saving/","title":"Data Loading and Saving","text":""},{"location":"data_loading_and_saving/#abcd_tools.utils.ConfigLoader","title":"<code>abcd_tools.utils.ConfigLoader</code>","text":"<p>YAML configuration loader utilities.</p>"},{"location":"data_loading_and_saving/#abcd_tools.utils.ConfigLoader.load_yaml","title":"<code>load_yaml(fpath)</code>","text":"<p>Load YAML configuration file.</p> Source code in <code>abcd_tools/utils/ConfigLoader.py</code> <pre><code>def load_yaml(fpath: str) -&gt; dict:\n    \"\"\"Load YAML configuration file.\"\"\"\n    fpath = pathlib.PurePath(fpath)\n    with open (fpath, 'r') as file:\n        conf = yaml.safe_load(file)\n\n    return conf\n</code></pre>"},{"location":"data_loading_and_saving/#abcd_tools.utils.io","title":"<code>abcd_tools.utils.io</code>","text":"<p>ABCD data I/O utilities.</p>"},{"location":"data_loading_and_saving/#abcd_tools.utils.io.apply_nda_names","title":"<code>apply_nda_names(df, abcd_dict_path='./abcd_5-1_dictionary.csv')</code>","text":"<p>Replace new names with old.</p> Source code in <code>abcd_tools/utils/io.py</code> <pre><code>def apply_nda_names(df: pd.DataFrame,\n                    abcd_dict_path: str=\"./abcd_5-1_dictionary.csv\") -&gt; pd.DataFrame:\n    '''Replace new names with old.'''\n\n    abcd_dict = pd.read_csv(abcd_dict_path)\n\n    abcd_dict = abcd_dict.set_index('var_name')['var_name_nda']\n    abcd_dict = abcd_dict.dropna().to_dict()\n\n    return df.rename(columns=abcd_dict)\n</code></pre>"},{"location":"data_loading_and_saving/#abcd_tools.utils.io.load_tabular","title":"<code>load_tabular(fpath, cols=None, timepoints=None, index=['src_subject_id', 'eventname'])</code>","text":"<p>Load tabular data file.</p> Source code in <code>abcd_tools/utils/io.py</code> <pre><code>def load_tabular(fpath: str,\n                 cols: list=None,\n                 timepoints: dict=None,\n                 index: list=['src_subject_id', 'eventname']\n                 ) -&gt; pd.DataFrame:\n    \"\"\"Load tabular data file.\"\"\"\n    df = pd.read_csv(fpath)\n\n    if timepoints:\n        df = df[df['eventname'].isin(timepoints)]\n\n    df.set_index(index, inplace=True)\n\n    if isinstance(cols, dict):\n        df = df[df.columns.intersection(set(cols.keys()))]\n        df = df.rename(columns=cols)\n    elif isinstance(cols, list):\n        df = df[df.columns.intersection(set(cols))]\n\n    return df\n</code></pre>"},{"location":"data_loading_and_saving/#abcd_tools.utils.io.parse_vol_info","title":"<code>parse_vol_info(vol_info)</code>","text":"<p>Parse volume info to get subject ID and timepoint.</p> <p>Parameters:</p> Name Type Description Default <code>vol_info</code> <code>DataFrame</code> <p>Volume info table.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Parsed subject ID and timepoint.</p> Source code in <code>abcd_tools/utils/io.py</code> <pre><code>def parse_vol_info(vol_info: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Parse volume info to get subject ID and timepoint.\n\n    Args:\n        vol_info (pd.DataFrame): Volume info table.\n\n    Returns:\n        pd.DataFrame: Parsed subject ID and timepoint.\n    \"\"\"\n\n    TPT_MAP = {\n        \"baseline\": \"baseline_year_1_arm_1\",\n        \"2year\": \"2_year_follow_up_y_arm_1\",\n        \"4year\": \"4_year_follow_up_y_arm_1\",\n        \"6year\": \"6_year_follow_up_y_arm_1\",\n    }\n\n    tmp = vol_info.iloc[:, 0].str.split(\"_\", expand=True)[[2, 3]]\n    tmp.columns = [\"src_subject_id\", \"eventname\"]\n    tmp[\"src_subject_id\"] = \"NDAR_\" + tmp[\"src_subject_id\"]\n    tmp[\"eventname\"] = tmp[\"eventname\"].map(TPT_MAP)\n\n    return tmp\n</code></pre>"},{"location":"data_loading_and_saving/#abcd_tools.utils.io.save_csv","title":"<code>save_csv(df, path)</code>","text":"<p>Save DataFrame to CSV file.</p> Source code in <code>abcd_tools/utils/io.py</code> <pre><code>def save_csv(df: pd.DataFrame, path: str):\n    '''Save DataFrame to CSV file.'''\n    df.to_csv(path)\n</code></pre>"},{"location":"data_loading_and_saving/#abcd_tools.download.NDAFastTrack","title":"<code>abcd_tools.download.NDAFastTrack</code>","text":"<p>Selectively download ABCD Study FastTrack data from NDA.</p> <p>This is a wrapper around <code>nda-tools</code>, but allows the user to choose which subjects, time points, and data types to download.</p> <p>Typical usage example:</p> <pre><code>subjects=['NDARINVxxxxxxxx']\ntasks=['MID']\ntimepoints=['baselineYear1Arm1']\n\nmanifest_downloader = ManifestDownloader(dp=data_package, username=username)\nmanifest_downloader.download()\n\nparser = ManifestParser(data_package, subjects, tasks, timepoints)\nparser.parse()\nparser.write_s3_links()\n\ndownloader = Downloader(dp=data_package, username=username)\ndownloader.download()\n</code></pre>"},{"location":"data_loading_and_saving/#abcd_tools.download.NDAFastTrack.AbstractDownloader","title":"<code>AbstractDownloader</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base downloader class.</p> Source code in <code>abcd_tools/base.py</code> <pre><code>class AbstractDownloader(ABC):\n    \"\"\"Base downloader class.\"\"\"\n    @abstractmethod\n    def download():\n        \"\"\"Abstract download method.\"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"data_loading_and_saving/#abcd_tools.download.NDAFastTrack.AbstractDownloader.download","title":"<code>download()</code>  <code>abstractmethod</code>","text":"<p>Abstract download method.</p> Source code in <code>abcd_tools/base.py</code> <pre><code>@abstractmethod\ndef download():\n    \"\"\"Abstract download method.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"data_loading_and_saving/#abcd_tools.download.NDAFastTrack.AbstractParser","title":"<code>AbstractParser</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base parser class.</p> Source code in <code>abcd_tools/base.py</code> <pre><code>class AbstractParser(ABC):\n    \"\"\"Base parser class.\"\"\"\n    @abstractmethod\n    def parse():\n        \"\"\"Abstract parse method.\"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"data_loading_and_saving/#abcd_tools.download.NDAFastTrack.AbstractParser.parse","title":"<code>parse()</code>  <code>abstractmethod</code>","text":"<p>Abstract parse method.</p> Source code in <code>abcd_tools/base.py</code> <pre><code>@abstractmethod\ndef parse():\n    \"\"\"Abstract parse method.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"data_loading_and_saving/#abcd_tools.download.NDAFastTrack.Downloader","title":"<code>Downloader</code>","text":"<p>               Bases: <code>AbstractDownloader</code></p> <p>Downloader object to interface with <code>nda-tools</code> <code>downloadcmd</code>.</p> <p>Attributes:</p> Name Type Description <code>dp</code> <code>str</code> <p>NDA Data Package ID.</p> <code>username</code> <code>str</code> <p>NDA account username. Defaults to None.</p> <code>download_directory</code> <code>(str, PathLike)</code> <p>Path to download location. Defaults to \".\"</p> Source code in <code>abcd_tools/download/NDAFastTrack.py</code> <pre><code>class Downloader(AbstractDownloader):\n    \"\"\"Downloader object to interface with `nda-tools` `downloadcmd`.\n\n    Attributes:\n        dp (str, optional): NDA Data Package ID.\n        username (str, optional): NDA account username. Defaults to None.\n        download_directory (str, os.PathLike): Path to download location.\n            Defaults to \".\"\n    \"\"\"\n    def __init__(   # noqa: DOC301\n            self, dp: str, username: str=None,\n            download_directory: str | os.PathLike=\".\"\n    ):\n        self.dp = dp\n        self.username = username\n        self.download_directory = download_directory\n\n    def download(self, s3_links: str | os.PathLike=\"s3_links.csv\"):\n        \"\"\"Make `subprocess` call to `nda-tools` `downloadcmd`.\n\n        Args:\n            s3_links (str | os.PathLike): Path to download location.\n                Defaults to \"s3_links.csv\"\n        \"\"\"\n\n        subprocess.run([\n            'downloadcmd',\n            '-dp', self.dp,\n            '-t', s3_links,\n            '-u', self.username,\n            '-d', self.download_directory\n        ])\n</code></pre>"},{"location":"data_loading_and_saving/#abcd_tools.download.NDAFastTrack.Downloader.download","title":"<code>download(s3_links='s3_links.csv')</code>","text":"<p>Make <code>subprocess</code> call to <code>nda-tools</code> <code>downloadcmd</code>.</p> <p>Parameters:</p> Name Type Description Default <code>s3_links</code> <code>str | PathLike</code> <p>Path to download location. Defaults to \"s3_links.csv\"</p> <code>'s3_links.csv'</code> Source code in <code>abcd_tools/download/NDAFastTrack.py</code> <pre><code>def download(self, s3_links: str | os.PathLike=\"s3_links.csv\"):\n    \"\"\"Make `subprocess` call to `nda-tools` `downloadcmd`.\n\n    Args:\n        s3_links (str | os.PathLike): Path to download location.\n            Defaults to \"s3_links.csv\"\n    \"\"\"\n\n    subprocess.run([\n        'downloadcmd',\n        '-dp', self.dp,\n        '-t', s3_links,\n        '-u', self.username,\n        '-d', self.download_directory\n    ])\n</code></pre>"},{"location":"data_loading_and_saving/#abcd_tools.download.NDAFastTrack.FastTrackReorganizer","title":"<code>FastTrackReorganizer</code>","text":"<p>Reorganizer of downloaded NDA FastTrack files.</p> <p>Attributes:</p> Name Type Description <code>source</code> <code>str | PathLike</code> <p>Source file directory.</p> <code>target</code> <code>str | PathLike</code> <p>Destination file directory.</p> Source code in <code>abcd_tools/download/NDAFastTrack.py</code> <pre><code>class FastTrackReorganizer():\n    \"\"\"Reorganizer of downloaded NDA FastTrack files.\n\n    Attributes:\n        source (str | os.PathLike): Source file directory.\n        target (str | os.PathLike): Destination file directory.\n    \"\"\"\n    def __init__(self, source: str | os.PathLike, target: str | os.PathLike):\n        self.source = source\n        self.target = target\n\n    def reorganize(self):\n        \"\"\"Extract (and combine) compressed BIDS directories (default method).\n        \"\"\"\n        self.reorganize_compressed()\n\n    def reorganize_compressed(self):\n        \"\"\"Extract (and combine) compressed BIDS directories.\n        \"\"\"\n\n        for root, dirs, files in os.walk(self.source):\n            for file in files:\n                fpath = os.path.join(root, file)\n                if tarfile.is_tarfile(fpath):\n                    with tarfile.open(fpath) as tf:\n                        # see tarfile.data_filter() for details\n                        tf.extractall(\n                            self.target, filter=tarfile.data_filter)\n\n    def reorganize_uncompressed(self, move=True):\n        \"\"\"Walk through subdirectories and move or copy them into BIDS compliance.\n\n        Args:\n            move (bool, optional): Move files to target; copy if False.\n                Defaults to True.\n        \"\"\"\n        for root, dirs, _ in os.walk(self.source):\n            for dir in dirs:\n                if dir.startswith('sub-'):\n                    walker = os.walk(root)\n                    next(walker) # skip top-level directory\n                    self._walk_path(walker, root, move)\n\n    def _walk_path(self, walker: Generator, root: str | os.PathLike, move: bool):\n        \"\"\"Iteratively walk through subdirectories and move or copy files.\n\n        Args:\n            walker (Generator): Generated from os.walk()\n            root (str | os.PathLike): Source location.\n            move (bool): Move files to target; copy if False.\n        \"\"\"\n\n        for r, _, files in walker:\n            newpath = r.replace(root + '/', '')\n            destpath = os.path.join(self.target, newpath)\n\n            if not os.path.exists(destpath):\n                os.mkdir(destpath)\n\n            for file in files:\n                sourcepath = os.path.join(r, file)\n\n                if move:\n                    self._move(sourcepath, destpath)\n                else:\n                    self._copy(sourcepath, destpath)\n\n    def _move(self, sourcepath: str | os.PathLike, destpath: str | os.PathLike):\n        \"\"\"Move file from sourcepath to destpath.\n\n        Args:\n            sourcepath (str | os.PathLike): File to be moved.\n            destpath (str | os.PathLike): File destination directory.\n        \"\"\"\n        try:\n            shutil.move(sourcepath, destpath)\n        except shutil.Error:\n            path = PurePath(sourcepath)\n            print(f'{os.path.join(destpath, path.name)} already exists')\n\n    def _copy(self, sourcepath: str | os.PathLike, destpath: str | os.PathLike):\n        \"\"\"Copy file from sourcepath to destpath.\n\n        Args:\n            sourcepath (str | os.PathLike): File to be copied.\n            destpath (str | os.PathLike): File destination directory.\n        \"\"\"\n        shutil.copy2(sourcepath, destpath)\n</code></pre>"},{"location":"data_loading_and_saving/#abcd_tools.download.NDAFastTrack.FastTrackReorganizer.reorganize","title":"<code>reorganize()</code>","text":"<p>Extract (and combine) compressed BIDS directories (default method).</p> Source code in <code>abcd_tools/download/NDAFastTrack.py</code> <pre><code>def reorganize(self):\n    \"\"\"Extract (and combine) compressed BIDS directories (default method).\n    \"\"\"\n    self.reorganize_compressed()\n</code></pre>"},{"location":"data_loading_and_saving/#abcd_tools.download.NDAFastTrack.FastTrackReorganizer.reorganize_compressed","title":"<code>reorganize_compressed()</code>","text":"<p>Extract (and combine) compressed BIDS directories.</p> Source code in <code>abcd_tools/download/NDAFastTrack.py</code> <pre><code>def reorganize_compressed(self):\n    \"\"\"Extract (and combine) compressed BIDS directories.\n    \"\"\"\n\n    for root, dirs, files in os.walk(self.source):\n        for file in files:\n            fpath = os.path.join(root, file)\n            if tarfile.is_tarfile(fpath):\n                with tarfile.open(fpath) as tf:\n                    # see tarfile.data_filter() for details\n                    tf.extractall(\n                        self.target, filter=tarfile.data_filter)\n</code></pre>"},{"location":"data_loading_and_saving/#abcd_tools.download.NDAFastTrack.FastTrackReorganizer.reorganize_uncompressed","title":"<code>reorganize_uncompressed(move=True)</code>","text":"<p>Walk through subdirectories and move or copy them into BIDS compliance.</p> <p>Parameters:</p> Name Type Description Default <code>move</code> <code>bool</code> <p>Move files to target; copy if False. Defaults to True.</p> <code>True</code> Source code in <code>abcd_tools/download/NDAFastTrack.py</code> <pre><code>def reorganize_uncompressed(self, move=True):\n    \"\"\"Walk through subdirectories and move or copy them into BIDS compliance.\n\n    Args:\n        move (bool, optional): Move files to target; copy if False.\n            Defaults to True.\n    \"\"\"\n    for root, dirs, _ in os.walk(self.source):\n        for dir in dirs:\n            if dir.startswith('sub-'):\n                walker = os.walk(root)\n                next(walker) # skip top-level directory\n                self._walk_path(walker, root, move)\n</code></pre>"},{"location":"data_loading_and_saving/#abcd_tools.download.NDAFastTrack.ManifestDownloader","title":"<code>ManifestDownloader</code>","text":"<p>               Bases: <code>Downloader</code></p> <p><code>nda-tools</code> downloader object.</p> <p>Attributes:</p> Name Type Description <code>dp</code> <code>str</code> <p>NDA Data Package ID.</p> <code>username</code> <code>str</code> <p>NDA username.</p> Source code in <code>abcd_tools/download/NDAFastTrack.py</code> <pre><code>class ManifestDownloader(Downloader):\n    \"\"\"`nda-tools` downloader object.\n\n    Attributes:\n        dp (str): NDA Data Package ID.\n        username (str): NDA username.\n    \"\"\"\n    def __init__(self, dp: str, username: str=None):\n        self.dp = dp\n        self.username = username\n\n    def download(self) -&gt; None:\n       \"\"\"\n       Make an empty query through `nda-tools` to retrieve\n       manifest file.\n       \"\"\"\n       null_fp = self._write_null_file()\n       downloader = Downloader(dp=self.dp, username=self.username)\n       downloader.download(s3_links=null_fp)\n\n    # TODO remove reliance on text file\n    def _write_null_file(self, dir: str | os.PathLike=\".\"):\n        \"\"\"Create empty text file to be passed as `s3_links`.\n\n        Args:\n            dir (str | os.PathLike, optional): File directory. Defaults to \".\"\n\n        Returns:\n            filename: Directory of null file.\n        \"\"\"\n        fname = os.path.join(dir, 'none.txt')\n        with open(fname, 'w') as f:\n            f.write('')\n        return fname\n</code></pre>"},{"location":"data_loading_and_saving/#abcd_tools.download.NDAFastTrack.ManifestDownloader.download","title":"<code>download()</code>","text":"<p>Make an empty query through <code>nda-tools</code> to retrieve manifest file.</p> Source code in <code>abcd_tools/download/NDAFastTrack.py</code> <pre><code>def download(self) -&gt; None:\n   \"\"\"\n   Make an empty query through `nda-tools` to retrieve\n   manifest file.\n   \"\"\"\n   null_fp = self._write_null_file()\n   downloader = Downloader(dp=self.dp, username=self.username)\n   downloader.download(s3_links=null_fp)\n</code></pre>"},{"location":"data_loading_and_saving/#abcd_tools.download.NDAFastTrack.ManifestParser","title":"<code>ManifestParser</code>","text":"<p>               Bases: <code>AbstractParser</code></p> <p><code>nda-tools</code> manifest parser object.</p> <p>Attributes:</p> Name Type Description <code>dp</code> <code>str</code> <p>NDA Data Package ID.</p> <code>subjects</code> <code>list</code> <p>List of subject IDs (e.g., ['NDARINVxxxxxxxx']). Defaults to None.</p> <code>timepoints</code> <code>list</code> <p>List of REDCap-style time points (e.g., ['baselineYear1Arm1']). Defaults to None.</p> <code>tasks</code> <code>list</code> <p>List of tasks (e.g., ['MID']). Defaults to None.</p> <code>metadata_filepath</code> <code>str | PathLike</code> <p>Path to user-supplied data manifest. Defaults to None.</p> Source code in <code>abcd_tools/download/NDAFastTrack.py</code> <pre><code>class ManifestParser(AbstractParser):\n    \"\"\" `nda-tools` manifest parser object.\n\n    Attributes:\n        dp (str): NDA Data Package ID.\n        subjects (list, optional): List of subject IDs (e.g., ['NDARINVxxxxxxxx']).\n            Defaults to None.\n        timepoints (list, optional): List of REDCap-style time points\n            (e.g., ['baselineYear1Arm1']). Defaults to None.\n        tasks (list, optional): List of tasks (e.g., ['MID']). Defaults to None.\n        metadata_filepath (str | os.PathLike, optional): Path to user-supplied\n            data manifest. Defaults to None.\n    \"\"\"\n    def __init__(\n            self, dp: str, subjects: list=None, timepoints: list=None,\n            tasks: list=None,\n            metadata_filepath: str | os.PathLike=None\n    ):\n        self.dp = dp\n        self.subjects = subjects\n        self.timepoints = timepoints\n        self.tasks=tasks\n        self.metadata_filepath = metadata_filepath\n        self.metadata = None\n        self.s3_links = None\n\n\n    def parse(self):\n        \"\"\"Parse metadata and generate S3 links.\n        \"\"\"\n        self.load_metadata()\n        self._parse_metadata()\n        self.get_s3_links_from_metadata()\n\n    def load_metadata(self):\n        \"\"\"Loads metadata from file.\n\n        If MetadataParser.metadata_filepath is not speecified,\n        attempt to load from default location.\n\n        Raises:\n            FileNotFoundError: Could not find manifest file.\n        \"\"\"\n\n        fpath = self.metadata_filepath\n        if not fpath:\n            base_path=os.path.expanduser(\"~/NDA/nda-tools/downloadcmd/packages\")\n            fname = f'{self.dp}/package_file_metadata_{self.dp}.txt'\n            fpath = os.path.join(base_path, fname)\n\n        if not os.path.isfile(fpath):\n            msg = f\"Could not find manifest file at {fpath}\"\n            raise FileNotFoundError(msg)\n\n        self.metadata = pd.read_csv(fpath)\n\n\n    def _parse_metadata(self):\n        \"\"\"Break `DOWNLOAD_ALIAS` column into meaningful chunks to query.\n        \"\"\"\n\n        def _parse_task(s: pd.Series) -&gt; pd.Series:\n            \"\"\"Extract fMRI task name.\n\n            Returns:\n                pd.Series: Standardized task names, if available.\n            \"\"\"\n            rep_strings = ['ABCD-MPROC-', '-fMRI']\n            for string in rep_strings:\n                s = s.str.replace(string, '')\n            return s\n\n        names = ['src_subject_id', 'eventname', 'task', 'date']\n\n        alias = self.metadata['DOWNLOAD_ALIAS'].str.split('/', expand=True)\n        alias = alias[2].str.split('_', expand=True)\n        alias.columns = names\n\n        alias['task'] = _parse_task(alias['task'])\n        self.metadata = pd.concat([self.metadata, alias], axis=1)\n\n    def get_s3_links_from_metadata(self):\n        \"\"\"Get S3 links for requested data.\n\n        Filter for subjects, timepoints, and tasks if supplied, otherwise\n        return all.\n        \"\"\"\n\n        def _clean_subjects(subjects: list) -&gt; list:\n            \"\"\"Standardize list of subject IDs.\"\"\"\n            if subjects:\n                subjects = [s.replace('_', '') for s in subjects]\n                subjects = [s.replace('sub-', '') for s in subjects]\n            return subjects\n\n        filter_args = {\n            'src_subject_id': _clean_subjects(self.subjects),\n            'eventname': self.timepoints,\n            'task': self.tasks\n        }\n\n        for k, v in filter_args.items():\n            if v:\n                self.metadata = self.metadata[self.metadata[k].isin(v)]\n\n        self.s3_links = self.metadata['NDA_S3_URL']\n\n    def write_s3_links(self, fname: str | os.PathLike):\n        \"\"\"Write list of S3 links to a text file.\n\n        Args:\n            fname (str | os.PathLike): Destination text file (.csv).\n        \"\"\"\n        try:\n            self.s3_links.to_csv(fname, header=False, index=False)\n        except Exception:\n            print(traceback.format_exc())\n</code></pre>"},{"location":"data_loading_and_saving/#abcd_tools.download.NDAFastTrack.ManifestParser.get_s3_links_from_metadata","title":"<code>get_s3_links_from_metadata()</code>","text":"<p>Get S3 links for requested data.</p> <p>Filter for subjects, timepoints, and tasks if supplied, otherwise return all.</p> Source code in <code>abcd_tools/download/NDAFastTrack.py</code> <pre><code>def get_s3_links_from_metadata(self):\n    \"\"\"Get S3 links for requested data.\n\n    Filter for subjects, timepoints, and tasks if supplied, otherwise\n    return all.\n    \"\"\"\n\n    def _clean_subjects(subjects: list) -&gt; list:\n        \"\"\"Standardize list of subject IDs.\"\"\"\n        if subjects:\n            subjects = [s.replace('_', '') for s in subjects]\n            subjects = [s.replace('sub-', '') for s in subjects]\n        return subjects\n\n    filter_args = {\n        'src_subject_id': _clean_subjects(self.subjects),\n        'eventname': self.timepoints,\n        'task': self.tasks\n    }\n\n    for k, v in filter_args.items():\n        if v:\n            self.metadata = self.metadata[self.metadata[k].isin(v)]\n\n    self.s3_links = self.metadata['NDA_S3_URL']\n</code></pre>"},{"location":"data_loading_and_saving/#abcd_tools.download.NDAFastTrack.ManifestParser.load_metadata","title":"<code>load_metadata()</code>","text":"<p>Loads metadata from file.</p> <p>If MetadataParser.metadata_filepath is not speecified, attempt to load from default location.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>Could not find manifest file.</p> Source code in <code>abcd_tools/download/NDAFastTrack.py</code> <pre><code>def load_metadata(self):\n    \"\"\"Loads metadata from file.\n\n    If MetadataParser.metadata_filepath is not speecified,\n    attempt to load from default location.\n\n    Raises:\n        FileNotFoundError: Could not find manifest file.\n    \"\"\"\n\n    fpath = self.metadata_filepath\n    if not fpath:\n        base_path=os.path.expanduser(\"~/NDA/nda-tools/downloadcmd/packages\")\n        fname = f'{self.dp}/package_file_metadata_{self.dp}.txt'\n        fpath = os.path.join(base_path, fname)\n\n    if not os.path.isfile(fpath):\n        msg = f\"Could not find manifest file at {fpath}\"\n        raise FileNotFoundError(msg)\n\n    self.metadata = pd.read_csv(fpath)\n</code></pre>"},{"location":"data_loading_and_saving/#abcd_tools.download.NDAFastTrack.ManifestParser.parse","title":"<code>parse()</code>","text":"<p>Parse metadata and generate S3 links.</p> Source code in <code>abcd_tools/download/NDAFastTrack.py</code> <pre><code>def parse(self):\n    \"\"\"Parse metadata and generate S3 links.\n    \"\"\"\n    self.load_metadata()\n    self._parse_metadata()\n    self.get_s3_links_from_metadata()\n</code></pre>"},{"location":"data_loading_and_saving/#abcd_tools.download.NDAFastTrack.ManifestParser.write_s3_links","title":"<code>write_s3_links(fname)</code>","text":"<p>Write list of S3 links to a text file.</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str | PathLike</code> <p>Destination text file (.csv).</p> required Source code in <code>abcd_tools/download/NDAFastTrack.py</code> <pre><code>def write_s3_links(self, fname: str | os.PathLike):\n    \"\"\"Write list of S3 links to a text file.\n\n    Args:\n        fname (str | os.PathLike): Destination text file (.csv).\n    \"\"\"\n    try:\n        self.s3_links.to_csv(fname, header=False, index=False)\n    except Exception:\n        print(traceback.format_exc())\n</code></pre>"},{"location":"image_processing/","title":"Image Processing","text":""},{"location":"image_processing/#abcd_tools.image.preprocess","title":"<code>abcd_tools.image.preprocess</code>","text":"<p>fMRI preprocessing functions</p>"},{"location":"image_processing/#abcd_tools.image.preprocess.compute_average_betas","title":"<code>compute_average_betas(run1, run2, vol_info, motion, name, release='r6', rem_outliers=False, outlier_std_threshold=3, normalize=False)</code>","text":"<p>Compute average betas across two runs.</p> <p>Parameters:</p> Name Type Description Default <code>run1</code> <code>DataFrame</code> <p>Beta values for run 1.</p> required <code>run2</code> <code>DataFrame</code> <p>Beta values for run 2.</p> required <code>vol_info</code> <code>DataFrame</code> <p>Volume information.</p> required <code>motion</code> <code>DataFrame</code> <p>Motion information.</p> required <code>name</code> <code>str</code> <p>Name of the task.</p> required <code>release</code> <code>str</code> <p>Release version. Defaults to 'r6'.</p> <code>'r6'</code> <code>rem_outliers</code> <code>bool</code> <p>Remove outliers. Defaults to False.</p> <code>False</code> <code>outlier_std_threshold</code> <code>float</code> <p>Outlier threshold. Defaults to 3.</p> <code>3</code> <code>normalize</code> <code>bool</code> <p>Normalize by sum. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Average betas across two runs.</p> Source code in <code>abcd_tools/image/preprocess.py</code> <pre><code>def compute_average_betas(run1: pd.DataFrame, run2: pd.DataFrame,\n    vol_info: pd.DataFrame, motion: pd.DataFrame,\n    name: str, release='r6', rem_outliers: bool=False, outlier_std_threshold: float=3,\n    normalize: bool=False) -&gt; pd.DataFrame:\n\n    \"\"\"Compute average betas across two runs.\n\n    Args:\n        run1 (pd.DataFrame): Beta values for run 1.\n        run2 (pd.DataFrame): Beta values for run 2.\n        vol_info (pd.DataFrame): Volume information.\n        motion (pd.DataFrame): Motion information.\n        name (str): Name of the task.\n        release (str, optional): Release version. Defaults to 'r6'.\n        rem_outliers (bool, optional): Remove outliers. Defaults to False.\n        outlier_std_threshold (float, optional): Outlier threshold. Defaults to 3.\n        normalize (bool, optional): Normalize by sum. Defaults to False.\n\n    Returns:\n        pd.DataFrame: Average betas across two runs.\n    \"\"\"\n\n    run1 = pd.concat([run1, vol_info], axis=1)\n    run2 = pd.concat([run2, vol_info], axis=1)\n\n    if release == 'r5':\n        run1 = run1[run1['eventname'] == 'baseline_year_1_arm_1']\n        run2 = run2[run2['eventname'] == 'baseline_year_1_arm_1']\n\n        motion = motion.reset_index()\n        motion = motion[motion['eventname'] == 'baseline_year_1_arm_1']\n        motion = motion.set_index(['src_subject_id', 'eventname'])\n\n    def _align(run1, run2, motion):\n        \"\"\"Align dataframes on index and columns.\"\"\"\n        motion.columns = ['run1_dof', 'run2_dof']\n\n        run1, run2 = run1.align(run2, axis=1)\n        run1, motion = run1.align(motion, axis=0)\n        run2, motion = run2.align(motion, axis=0)\n\n        return run1, run2, motion\n\n    idx = ['src_subject_id', 'eventname']\n    run1 = run1.set_index(idx)\n    run2 = run2.set_index(idx)\n\n    if rem_outliers:\n        run1 = remove_outliers(run1, outlier_std_threshold)\n        run2 = remove_outliers(run2, outlier_std_threshold)\n\n    if normalize:\n        run1 = normalize_by_sum(run1)\n        run2 = normalize_by_sum(run2)\n\n    run1_stripped, run2_stripped, motion = _align(run1, run2, motion)\n\n    # Betas == 0 are not included in the average\n    run1_stripped[run1_stripped == 0] = np.nan\n    run2_stripped[run2_stripped == 0] = np.nan\n\n    # multiply Beta values by degrees of freedom\n    run1_weighted = run1_stripped.mul(motion['run1_dof'], axis=0)\n    run2_weighted = run2_stripped.mul(motion['run2_dof'], axis=0)\n\n    # divide sum by total degrees of freedom\n    num = run1_weighted.add(run2_weighted, axis=0)\n    den = motion['run1_dof'] + motion['run2_dof']\n    avg = num.div(den, axis=0)\n\n    avg.columns = [c.replace('tableData', name + '_') for c in avg.columns]\n\n   # remove columns and rows that are all missing\n    return avg.dropna(how='all', axis=1).dropna(how='all', axis=0)\n</code></pre>"},{"location":"image_processing/#abcd_tools.image.preprocess.map_hemisphere","title":"<code>map_hemisphere(vertices, mapping, labels, prefix=None, suffix=None, decode_ascii=True)</code>","text":"<p>Map tabular vertexwise fMRI values to ROIs using nonzero average aggregation.</p> <p>Parameters:</p> Name Type Description Default <code>vertices</code> <code>DataFrame</code> <p>Tabular vertexwise data (columns are vertices).</p> required <code>mapping</code> <code>array</code> <p>Array of ROI indices. Must be the same length as <code>vertices</code>.</p> required <code>labels</code> <code>list</code> <p>ROI labels for resulting averaged values.</p> required <code>prefix</code> <code>str</code> <p>Prefix added to all column names. Defaults to None.</p> <code>None</code> <code>suffix</code> <code>str</code> <p>Suffix added to all column names. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Nonzero-averaged ROIs.</p> Source code in <code>abcd_tools/image/preprocess.py</code> <pre><code>def map_hemisphere(vertices: pd.DataFrame, mapping: np.array, labels: list,\n                   prefix: str=None, suffix: str=None,\n                   decode_ascii: bool=True) -&gt; pd.DataFrame:\n    \"\"\"Map tabular vertexwise fMRI values to ROIs using nonzero average aggregation.\n\n    Args:\n        vertices (pd.DataFrame): Tabular vertexwise data (columns are vertices).\n        mapping (np.array): Array of ROI indices. Must be the same length as `vertices`.\n        labels (list): ROI labels for resulting averaged values.\n        prefix (str, optional): Prefix added to all column names. Defaults to None.\n        suffix (str, optional): Suffix added to all column names. Defaults to None.\n\n    Returns:\n        pd.DataFrame: Nonzero-averaged ROIs.\n    \"\"\"\n\n    if decode_ascii:\n        labels = [label.decode() for label in labels]\n\n    map_dict = {}\n\n    for idx in mapping:\n\n        vertex = vertices.values[:, idx]\n\n        if idx in map_dict:\n            joined = np.array([map_dict[idx], vertex])\n            masked = np.ma.masked_equal(joined, 0) # only compute mean of nonzeros\n            map_dict[idx] = np.nanmean( masked, axis=0)\n        else:\n            map_dict[idx] = vertex\n\n    rois = pd.DataFrame(map_dict)\n    rois = rois.reindex(sorted(rois.columns), axis=1)\n\n    if len(labels) &gt; rois.shape[1]:\n        labels = labels[1:]\n\n    labels = [prefix + str(label) + suffix for label in labels]\n    rois.columns = labels\n    return rois\n</code></pre>"},{"location":"image_processing/#abcd_tools.image.preprocess.normalize_by_sum","title":"<code>normalize_by_sum(df)</code>","text":"<p>Normalize dataframe by the sum of absolute values within each row.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe to normalize</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Normalized dataframe</p> Source code in <code>abcd_tools/image/preprocess.py</code> <pre><code>def normalize_by_sum(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Normalize dataframe by the sum of absolute values within each row.\n\n    Args:\n        df (pd.DataFrame): Dataframe to normalize\n\n    Returns:\n        pd.DataFrame: Normalized dataframe\n    \"\"\"\n\n    def sum_nonzero_abs(row):\n        \"\"\"Sum the absolute values of non-zero elements in a row.\"\"\"\n        # Get absolute values\n        abs_vals = np.abs(row)\n        # Select only non-zero values and sum them\n        return np.sum(abs_vals[abs_vals != 0])\n\n    # Apply the function to each row to get the divisors\n    divisors = df.apply(sum_nonzero_abs, axis=1)\n\n    # Divide each row by its corresponding divisor\n    normalized_df = df.div(divisors, axis=0)\n\n    return normalized_df\n</code></pre>"},{"location":"image_processing/#abcd_tools.image.preprocess.remove_outliers","title":"<code>remove_outliers(df, std_threshold)</code>","text":"<p>Dynamically winsorize dataframe within columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe to winsorize</p> required <code>std_threshold</code> <code>float</code> <p>Threshold for winsorization</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Winsorized dataframe</p> Source code in <code>abcd_tools/image/preprocess.py</code> <pre><code>def remove_outliers(df: pd.DataFrame, std_threshold: float) -&gt; pd.DataFrame:\n    \"\"\"Dynamically winsorize dataframe within columns.\n\n    Args:\n        df (pd.DataFrame): Dataframe to winsorize\n        std_threshold (float): Threshold for winsorization\n\n    Returns:\n        pd.DataFrame: Winsorized dataframe\n    \"\"\"\n\n    means = df.mean()\n    stds = df.std()\n\n    lower_bounds = means - std_threshold * stds\n    upper_bounds = means + std_threshold * stds\n\n    return df.clip(lower=lower_bounds, upper=upper_bounds, axis=1)\n</code></pre>"},{"location":"task_behavioral_processing/","title":"Task-Behavioral Processing","text":""},{"location":"task_behavioral_processing/#abcd_tools.task.behavior","title":"<code>abcd_tools.task.behavior</code>","text":"<p>Parse individual ePrime files.</p>"},{"location":"task_behavioral_processing/#abcd_tools.task.behavior.AbstractDataset","title":"<code>AbstractDataset</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base dataset class.</p> Source code in <code>abcd_tools/base.py</code> <pre><code>class AbstractDataset(ABC):\n    \"\"\"Base dataset class.\"\"\"\n    @abstractmethod\n    def load():\n        \"\"\"Abstract loading method.\"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"task_behavioral_processing/#abcd_tools.task.behavior.AbstractDataset.load","title":"<code>load()</code>  <code>abstractmethod</code>","text":"<p>Abstract loading method.</p> Source code in <code>abcd_tools/base.py</code> <pre><code>@abstractmethod\ndef load():\n    \"\"\"Abstract loading method.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"task_behavioral_processing/#abcd_tools.task.behavior.eprimeDataSet","title":"<code>eprimeDataSet</code>","text":"<p>               Bases: <code>AbstractDataset</code></p> <p>Initialize eprimeDataSet class.</p> <p>Attributes:</p> Name Type Description <code>filepath</code> <code>str | PathLike</code> <p>Path to ePrime file.</p> <code>cols</code> <code>list</code> <p>User-specified columns to load. Defaults to YAML set.</p> <code>sep</code> <code>str</code> <p>Delimeter. Defaults to '   '.</p> Source code in <code>abcd_tools/task/behavior.py</code> <pre><code>class eprimeDataSet(AbstractDataset):\n    \"\"\"Initialize eprimeDataSet class.\n\n    Attributes:\n        filepath (str | os.PathLike): Path to ePrime file.\n        cols (list, optional): User-specified columns to load. Defaults to YAML set.\n        sep (str, optional): Delimeter. Defaults to '\\t'.\n    \"\"\"\n    def __init__(self, filepath: str | os.PathLike, cols: list|str='default',\n    sep: str='\\t'\n    ):\n\n        self.filepath = pathlib.PurePath(filepath)\n        self.fname = self.filepath.name\n        self.taskname = self._find_taskname()\n        self.cols = cols\n        self.sep = sep\n\n    def load(self) -&gt; pd.DataFrame:\n        \"\"\"Load ePrime with Python.\n\n        Returns:\n            pd.DataFrame: ePrime events file.\n        \"\"\"\n\n        def _read_csv(fpath, sep, cols) -&gt; pd.DataFrame:\n            \"\"\"Helper function to read file with differing arguments.\"\"\"\n            return pd.read_csv(fpath, sep=sep, usecols=cols, engine=\"python\")\n\n        if self.cols is None:\n            df = _read_csv(self.filepath, self.sep, None)\n        if self.cols == 'default':\n            cols = self._load_columns()\n            df = _read_csv(self.filepath, self.sep, lambda c: c in set(cols))\n        else:\n            cols = self.cols\n            df = _read_csv(self.filepath, self.sep, cols)\n\n        df = self._insert_id_vars(df)\n\n        return df\n\n    def _load_columns(self, fpath: str=\"../conf/task.yaml\"):\n        \"\"\"Read columns for Python eprime loading.\n\n        Args:\n            fpath (str, optional): Location of YAML file specifying columns.\n                Defaults to \"../conf/task.yaml\".\n\n        Returns:\n            list: list of column names\n        \"\"\"\n        p = pathlib.Path(__file__).parents[1]\n        fpath = pathlib.Path(fpath)\n\n        config_path = p / fpath\n        colnames = ConfigLoader.load_yaml(config_path)\n        return colnames[self.taskname]\n\n    def _find_subjectid(self) -&gt; str:\n        \"\"\"Subset subject ID from filename.\"\"\"\n        return self.fname[0:16]\n\n    def _find_timepoint(self, s:str, e: str='fMRI') -&gt; str:\n        \"\"\"Subset timepoint from filename.\"\"\"\n        return re.findall(s+'_'+\"(.*)\"+'_'+e, self.fname)[0]\n\n    def _find_taskname(self, s: str='fMRI', e: str='task') -&gt; str:\n        \"\"\"Subset taskname from filename.\"\"\"\n        return re.findall(s+\"_\"+\"(.*)\"+\"_\"+e, self.fname)[0]\n\n    def _insert_id_vars(self, df) -&gt; pd.DataFrame:\n        \"\"\"Modify dataframe with file attributes.\"\"\"\n        subjectid = self._find_subjectid()\n        df.insert(0, 'src_subject_id', subjectid)\n        df.insert(1, 'eventname', self._find_timepoint(subjectid))\n        df.insert(2, 'task', self.taskname)\n\n        return df\n</code></pre>"},{"location":"task_behavioral_processing/#abcd_tools.task.behavior.eprimeDataSet.load","title":"<code>load()</code>","text":"<p>Load ePrime with Python.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: ePrime events file.</p> Source code in <code>abcd_tools/task/behavior.py</code> <pre><code>def load(self) -&gt; pd.DataFrame:\n    \"\"\"Load ePrime with Python.\n\n    Returns:\n        pd.DataFrame: ePrime events file.\n    \"\"\"\n\n    def _read_csv(fpath, sep, cols) -&gt; pd.DataFrame:\n        \"\"\"Helper function to read file with differing arguments.\"\"\"\n        return pd.read_csv(fpath, sep=sep, usecols=cols, engine=\"python\")\n\n    if self.cols is None:\n        df = _read_csv(self.filepath, self.sep, None)\n    if self.cols == 'default':\n        cols = self._load_columns()\n        df = _read_csv(self.filepath, self.sep, lambda c: c in set(cols))\n    else:\n        cols = self.cols\n        df = _read_csv(self.filepath, self.sep, cols)\n\n    df = self._insert_id_vars(df)\n\n    return df\n</code></pre>"},{"location":"task_behavioral_processing/#abcd_tools.task.behavior.eprimeProcessor","title":"<code>eprimeProcessor</code>","text":"<p>Initialize ePrime processor object.</p> <p>Parameters:</p> Name Type Description Default <code>taskname</code> <code>str</code> <p>One of {\"MID\", \"SST\", \"nBack\"}</p> required Source code in <code>abcd_tools/task/behavior.py</code> <pre><code>class eprimeProcessor():\n    \"\"\"Initialize ePrime processor object.\n\n    Args:\n        taskname (str): One of {\"MID\", \"SST\", \"nBack\"}\n    \"\"\"\n    def __init__(self, taskname: str):\n        self.taskname = taskname.lower()\n\n    def process(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Process extracted data.\n\n        Args:\n            data (pd.DataFrame): Event file from ePrimeDataSet.load().\n\n        Returns:\n            pd.DataFrame: Processed events.\n        \"\"\"\n\n        if self.taskname == \"mid\":\n            processed = self.MID_process(data)\n        elif self.taskname == \"sst\":\n            processed = self.SST_process(data)\n        elif self.taskname == \"nback\":\n            processed = self.nBack_process(data)\n\n        return processed\n\n    def _lowercase_columns(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Return dataframe with lowercase columnnames.\"\"\"\n        df.columns = [c.lower() for c in df.columns]\n        return df\n\n    def nBack_process(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Process nBack task events.\n\n        Args:\n            df (pd.DataFrame): Event file from ePrimeDataSet.load().\n\n        Returns:\n            pd.DataFrame: Processed data.\n        \"\"\"\n        idx = [\n            'src_subject_id',\n            'eventname',\n            'task',\n            'experimentname',\n            'trial_type',\n            'run',\n            'onsettime',\n            'offsettime',\n            'duration',\n            'stim.rt'\n        ]\n\n        rename = {'onsettime': 'onset'}\n        drop = ['offsettime']\n\n        df = self._lowercase_columns(df)\n\n        df = self._nback_impose_run(df)\n        df = self._nBack_drop_pre_dummy_scan(df)\n        df = self._nBack_align_timings(df)\n\n        cue_procs = ['cue0backproc', 'cue2backproc']\n        df.loc[df['procedure[block]'].isin(cue_procs), 'block.type'] = 'cue'\n        df['trial_type'] = df['blocktype'].combine_first(df['stimtype'])\n\n        df = self._nBack_merge_cue_stim(df)\n\n        df['duration'] = df['offsettime'] - df['onsettime']\n        df = (df[idx]\n              .dropna(subset='trial_type')\n              .rename(columns=rename)\n              .drop(columns=drop)\n        )\n\n        return df.reset_index(drop=True)\n\n    def _nback_impose_run(self, df: pd.DataFrame):\n        \"\"\"Explicitly indicate imaging run in events file.\"\"\"\n\n        switch_r1 = 'TRSyncPROC'\n        switch_r2 = 'TRSyncPROCR2'\n\n        df['run'] = np.where(df['procedure[block]'] == switch_r1, 1,\n            np.where(df['procedure[block]'] == switch_r2, 2,\n            np.nan\n        ))\n        df['run'] = df['run'].ffill()\n\n        return df\n\n    def _nBack_drop_pre_dummy_scan(self, df: pd.DataFrame):\n        \"\"\"Drop scans before indicator time.\"\"\"\n\n        prep_vars = ['getready.rttime', 'getready2.rttime']\n        if prep_vars[1] in list(df):\n            df[prep_vars[0]] = df[prep_vars[0]].combine_first(df[prep_vars[1]])\n\n        df[prep_vars[0]] = df[prep_vars[0]].ffill()\n\n        return df.dropna(subset=prep_vars[0])\n\n\n    def _nBack_align_timings(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Align timings by subtracting preparation time and converting to seconds.\"\"\"\n\n        # subtract prep time from onsettime/offsettime and convert to seconds\n        time_cols = [c for c in df.columns if 'onsettime' in c or 'offsettime' in c]\n        d = {cl: lambda x,\n                cl=cl: (x[cl] - x['getready.rttime']) / 1000\n                for cl in time_cols\n            }\n        df = df.assign(**d)\n\n        # convert reaction time to seconds\n        df['stim.rt'] = df['stim.rt'] / 1000\n\n        return df\n\n    def _nBack_merge_cue_stim(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Merge nBack task onsettimes and offsettimes.\"\"\"\n\n        df = df[df['procedure[block]'] != \"Fix15secPROC\"]\n\n        # add fixation and cue durations\n        df['cue.onsettime'] = df['cuefix.onsettime']\n        df['cue.offsettime'] = (df['cuetarget.offsettime']\n                                .combine_first(df['cue2back.offsettime'])\n        )\n\n        # merge columns\n\n        df['onsettime'] = df['cue.onsettime'].combine_first(df['stim.onsettime'])\n        df['offsettime'] = df['cue.offsettime'].combine_first(df['stim.offsettime'])\n\n        return df\n\n    def MID_process(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Process MID task events.\n\n        Args:\n            df (pd.DataFrame): Event file from eprimeDataset.load()\n\n        Returns:\n            pd.DataFrame: Processed events.\n        \"\"\"\n        #TODO place in config file\n        idx = [\n            'src_subject_id',\n            'eventname',\n            'task',\n            'experimentname',\n            # 'experimentversion',\n            'block',\n            'subtrial',\n            'condition'\n        ]\n        vars = [\n            'prbacc',\n            'runmoney',\n            # 'prbrt',\n            'overallrt',\n            'accuracy'\n        ]\n\n        times = [\n            'cue.onsettime',\n            'cue.offsettime',\n            # 'anticipation.onsettime',\n            # 'anticipation.offsettime',\n            'probe.onsettime',\n            'probe.offsettime',\n            'probeRT.onsettime',\n            'probeRT.offsettime',\n            'feedback.onsettime',\n            'feedback.offsettime'\n        ]\n\n        drop = ['condition','offsettime']\n        rename = {'block': 'run', 'onsettime': 'onset'}\n        df = self._lowercase_columns(df)\n\n        df = self._MID_drop_pre_dummy_scan(df)\n        df = self._MID_align_timings(df)\n        df = self._MID_compute_probeRT(df)\n        df = self._MID_parse_accuracy(df)\n\n        df = df[idx + times + vars]\n        return df\n\n        long = df.pivot_longer(\n            index=idx + vars,\n            names_to = ('trial_type', '.value'),\n            names_sep = '.'\n        )\n\n        long = self._MID_impose_trial_type(long)\n        long['duration'] = long['offsettime'] - long['onsettime']\n        long = long.rename(columns=rename)\n\n        return long.drop(columns=drop)\n\n\n    def _MID_parse_accuracy(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Impose explicit hit/miss.\"\"\"\n        df['accuracy'] = np.where(df['prbacc'] == 1, 'pos', 'neg')\n        return df\n\n    def _MID_impose_trial_type(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Parse basic trial types into specific ones.\"\"\"\n        df['trial_type'] = np.where(\n            df['trial_type'] == 'feedback',\n            df['condition'] + '_' + df['accuracy'] + '_' + df['trial_type'],\n            df['condition'] + '_' + df['trial_type']\n            )\n        return df\n\n    def _MID_drop_pre_dummy_scan(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Drop scans before indicator variable.\"\"\"\n        ready_var = 'getready.rttime'\n        prep_var = 'preptime.onsettime'\n        vars = [ready_var, prep_var]\n\n        for v in vars:\n            df[v] = df[v].ffill()\n\n        return df.dropna(subset=vars)\n\n    def _MID_align_timings(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Align timings by subtracting preparation time and converting to seconds.\"\"\"\n\n        # subtract prep time from onset/offset and convert to seconds\n        time_cols = [c for c in df.columns if 'onset' in c or 'offset' in c]\n        time_cols.remove('preptime.onsettime')\n        d = {cl: lambda x, cl=cl:\n             (x[cl] - x['preptime.onsettime']) / 1000 for cl in time_cols}\n        df = df.assign(**d)\n\n        if 'overallrt' in list(df):\n\n            # convert reaction time to seconds\n            df['overallrt'][df['overallrt'] == '?'] = np.nan # who knows\n            df['overallrt'] = df['overallrt'].astype('float')\n            df['overallrt'] = df['overallrt'] / 1000\n\n        else:\n            df['overallrt'] = np.nan\n\n        return df\n\n    def _MID_compute_probeRT(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Add Probe RT onset/offset.\"\"\"\n\n        df['probeRT.onsettime'] = df['probe.onsettime']\n        df['probeRT.offsettime'] = df['probeRT.onsettime'] + df['overallrt']\n\n        return df\n\n\n    # TODO SSRT is not calculated\n    # TODO figure out why onsets are off by 0.5 seconds\n    def SST_process(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Process SST events.\n\n        Args:\n            df (pd.DataFrame): Event file from ePrimeDataSet.load().\n\n        Returns:\n            pd.DataFrame: Processed events.\n        \"\"\"\n        SS_STIMULUS_TIME = 0.02\n\n        df = self._lowercase_columns(df)\n        df = self._SST_drop_pre_dummy_scan(df)\n        df = self._SST_align_timings(df)\n\n        idx = [\n            'src_subject_id',\n            'eventname',\n            'task',\n            'experimentname',\n            # 'experimentversion',\n            'trial',\n            'trialcode',\n            'onset',\n            'offset',\n            'duration'\n        ]\n\n        rename = {\n            'trial': 'run',\n            'trialcode': 'trial_type'\n        }\n\n        df['stopsignal.offsettime'] = df['stopsignal.starttime'] + SS_STIMULUS_TIME\n\n        df['onset'] = df['go.onsettime'].combine_first(df['stopsignal.starttime'])\n        df['offset'] = df['go.offsettime'].combine_first(df['stopsignal.offsettime'])\n\n        df['duration'] = df['offset'] - df['onset']\n        df = (df[idx]\n              .reset_index(drop=True)\n              .rename(columns=rename)\n            )\n        return df\n\n\n    def _SST_drop_pre_dummy_scan(self, df: pd.DataFrame):\n        \"\"\"Drop scans before indicator timing.\"\"\"\n\n        prep_var = 'beginfix.starttime'\n        indicator_var = 'fix.rt' # recorded for every trial\n        df[prep_var] = df[prep_var].ffill()\n\n        return df.dropna(subset=[prep_var, indicator_var])\n\n    def _SST_align_timings(self, df: pd.DataFrame):\n        \"\"\"Align times by subtracting preparation and converting to seconds.\"\"\"\n\n        time_stubs = ['onsettime', 'offsettime', 'starttime']\n        time_cols = [c for c in df.columns for s in time_stubs if s in c]\n        time_cols.remove('beginfix.starttime')\n\n        d = {cl: lambda x, cl=cl:\n             (x[cl] - x['beginfix.starttime']) / 1000 for cl in time_cols}\n        df = df.assign(**d)\n\n        dur_cols = ['fix.rt', 'go.rt', 'ssd.rt', 'stopsignal.rt']\n        d_dur = {cl: lambda x, cl=cl: (x[cl]) / 1000 for cl in dur_cols}\n\n        return df.assign(**d_dur)\n</code></pre>"},{"location":"task_behavioral_processing/#abcd_tools.task.behavior.eprimeProcessor.MID_process","title":"<code>MID_process(df)</code>","text":"<p>Process MID task events.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Event file from eprimeDataset.load()</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Processed events.</p> Source code in <code>abcd_tools/task/behavior.py</code> <pre><code>def MID_process(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Process MID task events.\n\n    Args:\n        df (pd.DataFrame): Event file from eprimeDataset.load()\n\n    Returns:\n        pd.DataFrame: Processed events.\n    \"\"\"\n    #TODO place in config file\n    idx = [\n        'src_subject_id',\n        'eventname',\n        'task',\n        'experimentname',\n        # 'experimentversion',\n        'block',\n        'subtrial',\n        'condition'\n    ]\n    vars = [\n        'prbacc',\n        'runmoney',\n        # 'prbrt',\n        'overallrt',\n        'accuracy'\n    ]\n\n    times = [\n        'cue.onsettime',\n        'cue.offsettime',\n        # 'anticipation.onsettime',\n        # 'anticipation.offsettime',\n        'probe.onsettime',\n        'probe.offsettime',\n        'probeRT.onsettime',\n        'probeRT.offsettime',\n        'feedback.onsettime',\n        'feedback.offsettime'\n    ]\n\n    drop = ['condition','offsettime']\n    rename = {'block': 'run', 'onsettime': 'onset'}\n    df = self._lowercase_columns(df)\n\n    df = self._MID_drop_pre_dummy_scan(df)\n    df = self._MID_align_timings(df)\n    df = self._MID_compute_probeRT(df)\n    df = self._MID_parse_accuracy(df)\n\n    df = df[idx + times + vars]\n    return df\n\n    long = df.pivot_longer(\n        index=idx + vars,\n        names_to = ('trial_type', '.value'),\n        names_sep = '.'\n    )\n\n    long = self._MID_impose_trial_type(long)\n    long['duration'] = long['offsettime'] - long['onsettime']\n    long = long.rename(columns=rename)\n\n    return long.drop(columns=drop)\n</code></pre>"},{"location":"task_behavioral_processing/#abcd_tools.task.behavior.eprimeProcessor.SST_process","title":"<code>SST_process(df)</code>","text":"<p>Process SST events.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Event file from ePrimeDataSet.load().</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Processed events.</p> Source code in <code>abcd_tools/task/behavior.py</code> <pre><code>def SST_process(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Process SST events.\n\n    Args:\n        df (pd.DataFrame): Event file from ePrimeDataSet.load().\n\n    Returns:\n        pd.DataFrame: Processed events.\n    \"\"\"\n    SS_STIMULUS_TIME = 0.02\n\n    df = self._lowercase_columns(df)\n    df = self._SST_drop_pre_dummy_scan(df)\n    df = self._SST_align_timings(df)\n\n    idx = [\n        'src_subject_id',\n        'eventname',\n        'task',\n        'experimentname',\n        # 'experimentversion',\n        'trial',\n        'trialcode',\n        'onset',\n        'offset',\n        'duration'\n    ]\n\n    rename = {\n        'trial': 'run',\n        'trialcode': 'trial_type'\n    }\n\n    df['stopsignal.offsettime'] = df['stopsignal.starttime'] + SS_STIMULUS_TIME\n\n    df['onset'] = df['go.onsettime'].combine_first(df['stopsignal.starttime'])\n    df['offset'] = df['go.offsettime'].combine_first(df['stopsignal.offsettime'])\n\n    df['duration'] = df['offset'] - df['onset']\n    df = (df[idx]\n          .reset_index(drop=True)\n          .rename(columns=rename)\n        )\n    return df\n</code></pre>"},{"location":"task_behavioral_processing/#abcd_tools.task.behavior.eprimeProcessor.nBack_process","title":"<code>nBack_process(df)</code>","text":"<p>Process nBack task events.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Event file from ePrimeDataSet.load().</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Processed data.</p> Source code in <code>abcd_tools/task/behavior.py</code> <pre><code>def nBack_process(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Process nBack task events.\n\n    Args:\n        df (pd.DataFrame): Event file from ePrimeDataSet.load().\n\n    Returns:\n        pd.DataFrame: Processed data.\n    \"\"\"\n    idx = [\n        'src_subject_id',\n        'eventname',\n        'task',\n        'experimentname',\n        'trial_type',\n        'run',\n        'onsettime',\n        'offsettime',\n        'duration',\n        'stim.rt'\n    ]\n\n    rename = {'onsettime': 'onset'}\n    drop = ['offsettime']\n\n    df = self._lowercase_columns(df)\n\n    df = self._nback_impose_run(df)\n    df = self._nBack_drop_pre_dummy_scan(df)\n    df = self._nBack_align_timings(df)\n\n    cue_procs = ['cue0backproc', 'cue2backproc']\n    df.loc[df['procedure[block]'].isin(cue_procs), 'block.type'] = 'cue'\n    df['trial_type'] = df['blocktype'].combine_first(df['stimtype'])\n\n    df = self._nBack_merge_cue_stim(df)\n\n    df['duration'] = df['offsettime'] - df['onsettime']\n    df = (df[idx]\n          .dropna(subset='trial_type')\n          .rename(columns=rename)\n          .drop(columns=drop)\n    )\n\n    return df.reset_index(drop=True)\n</code></pre>"},{"location":"task_behavioral_processing/#abcd_tools.task.behavior.eprimeProcessor.process","title":"<code>process(data)</code>","text":"<p>Process extracted data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Event file from ePrimeDataSet.load().</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Processed events.</p> Source code in <code>abcd_tools/task/behavior.py</code> <pre><code>def process(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Process extracted data.\n\n    Args:\n        data (pd.DataFrame): Event file from ePrimeDataSet.load().\n\n    Returns:\n        pd.DataFrame: Processed events.\n    \"\"\"\n\n    if self.taskname == \"mid\":\n        processed = self.MID_process(data)\n    elif self.taskname == \"sst\":\n        processed = self.SST_process(data)\n    elif self.taskname == \"nback\":\n        processed = self.nBack_process(data)\n\n    return processed\n</code></pre>"},{"location":"task_behavioral_processing/#abcd_tools.task.metrics","title":"<code>abcd_tools.task.metrics</code>","text":"<p>Compute various task metrics.</p>"},{"location":"task_behavioral_processing/#abcd_tools.task.metrics.AbstractDataset","title":"<code>AbstractDataset</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base dataset class.</p> Source code in <code>abcd_tools/base.py</code> <pre><code>class AbstractDataset(ABC):\n    \"\"\"Base dataset class.\"\"\"\n    @abstractmethod\n    def load():\n        \"\"\"Abstract loading method.\"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"task_behavioral_processing/#abcd_tools.task.metrics.AbstractDataset.load","title":"<code>load()</code>  <code>abstractmethod</code>","text":"<p>Abstract loading method.</p> Source code in <code>abcd_tools/base.py</code> <pre><code>@abstractmethod\ndef load():\n    \"\"\"Abstract loading method.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"task_behavioral_processing/#abcd_tools.task.metrics.DPrimeDataset","title":"<code>DPrimeDataset</code>","text":"<p>               Bases: <code>AbstractDataset</code></p> <p>Compute nBack task D-Prime.</p> Source code in <code>abcd_tools/task/metrics.py</code> <pre><code>class DPrimeDataset(AbstractDataset):\n    \"\"\"Compute nBack task D-Prime.\"\"\"\n\n    def __init__(self, columns_fpath: str=\"../conf/dprime.yaml\") -&gt; None:\n        self.columns = self._load_columns(columns_fpath)\n        pass\n\n    def load_and_compute(self, df, return_all=False) -&gt; pd.DataFrame:\n        \"\"\"Load nBack behavior metrics and compute dPrime metric.\n\n        `norminv = scipy.stats.norm.ppf`\n        d-prime = norminv(hit_rate) - norminv(false_alarm_rate)\n\n        Args:\n            df (_type_): nBack behavioral metrics dataframe.\n                (e.g., `mri_y_tfmr_nback_beh.csv`)\n            return_all (bool, optional): Return components used to compute dPrime.\n                Defaults to False.\n\n        Returns:\n            pd.DataFrame: Resulting dataframe with computed dPrime.\n        \"\"\"\n        nback_behavioral = self.load(df)\n        dprime = self.compute_dprime(nback_behavioral, return_all)\n        return dprime\n\n    def _load_columns(self, fpath: str) -&gt; dict:\n        \"\"\"Load MRI behavioral columns from configuration file.\n\n        Args:\n            fpath (str): YAML config filepath.\n\n        Returns:\n            dict: Column names.\n        \"\"\"\n        p = pathlib.Path(__file__).parents[1]\n        fpath = pathlib.Path(fpath)\n\n        config_path = p / fpath\n        vars = ConfigLoader.load_yaml(config_path)\n        return vars\n\n    def load(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Load and rename nBack d-prime components.\n\n        Args:\n            df (pd.DataFrame): nBack behavioral metrics dataframe.\n                (e.g., `mri_y_tfmr_nback_beh.csv`)\n\n        Returns:\n            pd.DataFrame: Metrics needed to compute nBack d-prime.\n        \"\"\"\n\n        df = df.set_index(['src_subject_id', 'eventname'])\n\n        result = pd.DataFrame()\n        for group, vars in self.columns.items():\n            tmp = df[vars.keys()]\n            tmp = tmp.rename(columns=vars)\n            result = pd.concat([result, tmp], axis=1)\n\n        return result\n\n    def compute_dprime(self, df: pd.DataFrame, return_all=False) -&gt; pd.DataFrame:\n        \"\"\"Compute n-Back 0-back and 2-back d-prime.\n\n        `norminv = scipy.stats.norm.ppf`\n        d-prime = norminv(hit_rate) - norminv(false_alarm_rate)\n\n        Args:\n            df (pd.DataFrame): d-prime components from `DPrimeDataset.load()`\n            return_all (bool, optional): Return components used to\n                compute d-prime. Defaults to False.\n\n        Returns:\n            pd.DataFrame: 0-back and 2-back d-prime values\n        \"\"\"\n        cols = self.columns\n\n        def _compute_rate(df: pd.DataFrame, n_correct: int, n_total:int) -&gt; float:\n            \"\"\"Helper function for rate computation.\"\"\"\n            correct = df[n_correct].sum(axis=1)\n            total = df[n_total].sum(axis=1)\n\n            return correct / total\n\n        target_correct_0back = cols['0back_target_correct'].values()\n        target_total_0back = cols['0back_target_total'].values()\n        reject_correct_0back = cols['0back_correct_reject'].values()\n        reject_total_0back = cols['0back_total_reject'].values()\n\n        target_correct_2back = cols['2back_target_correct'].values()\n        target_total_2back = cols['2back_target_total'].values()\n        reject_correct_2back = cols['2back_correct_reject'].values()\n        reject_total_2back = cols['2back_total_reject'].values()\n\n        hitrate_0back = _compute_rate(df, target_correct_0back, target_total_0back)\n        hitrate_2back = _compute_rate(df, target_correct_2back, target_total_2back)\n        f_alarm_0back = 1 - _compute_rate(df, reject_correct_0back, reject_total_0back)\n        f_alarm_2back = 1 - _compute_rate(df, reject_correct_2back, reject_total_2back)\n\n        dprime_0back = norm.ppf(hitrate_0back) - norm.ppf(f_alarm_0back)\n        dprime_2back = norm.ppf(hitrate_2back) - norm.ppf(f_alarm_2back)\n\n        df['dprime_0back'] = dprime_0back\n        df['dprime_2back'] = dprime_2back\n\n        if return_all:\n            return df\n        else:\n            return df[['dprime_0back', 'dprime_2back']]\n</code></pre>"},{"location":"task_behavioral_processing/#abcd_tools.task.metrics.DPrimeDataset.compute_dprime","title":"<code>compute_dprime(df, return_all=False)</code>","text":"<p>Compute n-Back 0-back and 2-back d-prime.</p> <p><code>norminv = scipy.stats.norm.ppf</code> d-prime = norminv(hit_rate) - norminv(false_alarm_rate)</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>d-prime components from <code>DPrimeDataset.load()</code></p> required <code>return_all</code> <code>bool</code> <p>Return components used to compute d-prime. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: 0-back and 2-back d-prime values</p> Source code in <code>abcd_tools/task/metrics.py</code> <pre><code>def compute_dprime(self, df: pd.DataFrame, return_all=False) -&gt; pd.DataFrame:\n    \"\"\"Compute n-Back 0-back and 2-back d-prime.\n\n    `norminv = scipy.stats.norm.ppf`\n    d-prime = norminv(hit_rate) - norminv(false_alarm_rate)\n\n    Args:\n        df (pd.DataFrame): d-prime components from `DPrimeDataset.load()`\n        return_all (bool, optional): Return components used to\n            compute d-prime. Defaults to False.\n\n    Returns:\n        pd.DataFrame: 0-back and 2-back d-prime values\n    \"\"\"\n    cols = self.columns\n\n    def _compute_rate(df: pd.DataFrame, n_correct: int, n_total:int) -&gt; float:\n        \"\"\"Helper function for rate computation.\"\"\"\n        correct = df[n_correct].sum(axis=1)\n        total = df[n_total].sum(axis=1)\n\n        return correct / total\n\n    target_correct_0back = cols['0back_target_correct'].values()\n    target_total_0back = cols['0back_target_total'].values()\n    reject_correct_0back = cols['0back_correct_reject'].values()\n    reject_total_0back = cols['0back_total_reject'].values()\n\n    target_correct_2back = cols['2back_target_correct'].values()\n    target_total_2back = cols['2back_target_total'].values()\n    reject_correct_2back = cols['2back_correct_reject'].values()\n    reject_total_2back = cols['2back_total_reject'].values()\n\n    hitrate_0back = _compute_rate(df, target_correct_0back, target_total_0back)\n    hitrate_2back = _compute_rate(df, target_correct_2back, target_total_2back)\n    f_alarm_0back = 1 - _compute_rate(df, reject_correct_0back, reject_total_0back)\n    f_alarm_2back = 1 - _compute_rate(df, reject_correct_2back, reject_total_2back)\n\n    dprime_0back = norm.ppf(hitrate_0back) - norm.ppf(f_alarm_0back)\n    dprime_2back = norm.ppf(hitrate_2back) - norm.ppf(f_alarm_2back)\n\n    df['dprime_0back'] = dprime_0back\n    df['dprime_2back'] = dprime_2back\n\n    if return_all:\n        return df\n    else:\n        return df[['dprime_0back', 'dprime_2back']]\n</code></pre>"},{"location":"task_behavioral_processing/#abcd_tools.task.metrics.DPrimeDataset.load","title":"<code>load(df)</code>","text":"<p>Load and rename nBack d-prime components.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>nBack behavioral metrics dataframe. (e.g., <code>mri_y_tfmr_nback_beh.csv</code>)</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Metrics needed to compute nBack d-prime.</p> Source code in <code>abcd_tools/task/metrics.py</code> <pre><code>def load(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Load and rename nBack d-prime components.\n\n    Args:\n        df (pd.DataFrame): nBack behavioral metrics dataframe.\n            (e.g., `mri_y_tfmr_nback_beh.csv`)\n\n    Returns:\n        pd.DataFrame: Metrics needed to compute nBack d-prime.\n    \"\"\"\n\n    df = df.set_index(['src_subject_id', 'eventname'])\n\n    result = pd.DataFrame()\n    for group, vars in self.columns.items():\n        tmp = df[vars.keys()]\n        tmp = tmp.rename(columns=vars)\n        result = pd.concat([result, tmp], axis=1)\n\n    return result\n</code></pre>"},{"location":"task_behavioral_processing/#abcd_tools.task.metrics.DPrimeDataset.load_and_compute","title":"<code>load_and_compute(df, return_all=False)</code>","text":"<p>Load nBack behavior metrics and compute dPrime metric.</p> <p><code>norminv = scipy.stats.norm.ppf</code> d-prime = norminv(hit_rate) - norminv(false_alarm_rate)</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>_type_</code> <p>nBack behavioral metrics dataframe. (e.g., <code>mri_y_tfmr_nback_beh.csv</code>)</p> required <code>return_all</code> <code>bool</code> <p>Return components used to compute dPrime. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Resulting dataframe with computed dPrime.</p> Source code in <code>abcd_tools/task/metrics.py</code> <pre><code>def load_and_compute(self, df, return_all=False) -&gt; pd.DataFrame:\n    \"\"\"Load nBack behavior metrics and compute dPrime metric.\n\n    `norminv = scipy.stats.norm.ppf`\n    d-prime = norminv(hit_rate) - norminv(false_alarm_rate)\n\n    Args:\n        df (_type_): nBack behavioral metrics dataframe.\n            (e.g., `mri_y_tfmr_nback_beh.csv`)\n        return_all (bool, optional): Return components used to compute dPrime.\n            Defaults to False.\n\n    Returns:\n        pd.DataFrame: Resulting dataframe with computed dPrime.\n    \"\"\"\n    nback_behavioral = self.load(df)\n    dprime = self.compute_dprime(nback_behavioral, return_all)\n    return dprime\n</code></pre>"}]}